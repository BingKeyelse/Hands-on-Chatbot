import os
import json
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain.schema import Document
from dotenv import load_dotenv
from uuid import uuid4
# from crawl import crawl_web
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader
)
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader
from bs4 import BeautifulSoup

import nltk
nltk.download('averaged_perceptron_tagger_eng')

def load_data_from_local(filename: str, directory: str):
    """
    Tá»± Ä‘á»™ng Ä‘á»c file vÄƒn báº£n theo Ä‘á»‹nh dáº¡ng báº±ng LangChain loader.
    Há»— trá»£: PDF, TXT, DOCX, CSV, MD
    """
    file_path = os.path.join(directory, filename)
    ext = filename.lower().split('.')[-1]

    # Chá»n loader phÃ¹ há»£p
    if ext == 'pdf':
        # loader = PyPDFLoader(file_path)
        loader = PyPDFLoader(file_path)
        docs = loader.load()

        # ğŸ§  Gá»™p toÃ n bá»™ ná»™i dung cÃ¡c trang láº¡i thÃ nh 1 document duy nháº¥t
        merged_text = "\n".join([d.page_content for d in docs])
        documents = [
            Document(
                page_content=merged_text,
                metadata={"source": file_path}
            )
        ]
    elif ext == 'txt':
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
    elif ext == 'docx':
        loader = UnstructuredWordDocumentLoader(file_path)
        documents = loader.load()
    elif ext == 'csv':
        loader = CSVLoader(file_path)
        documents = loader.load()
    elif ext == 'md':
        loader = UnstructuredMarkdownLoader(file_path)
        documents = loader.load()
    else:
        raise ValueError(f"âŒ Äá»‹nh dáº¡ng file '{ext}' chÆ°a Ä‘Æ°á»£c há»— trá»£.")

    # Load tÃ i liá»‡u (tráº£ vá» list Document)
    # documents = loader.load()
    return documents, filename, ext

def bs4_extractor(html: str) -> str:
    """
    HÃ m trÃ­ch xuáº¥t vÃ  lÃ m sáº¡ch ná»™i dung tá»« HTML
    Args:
        html: Chuá»—i HTML cáº§n xá»­ lÃ½
    Returns:
        str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch, loáº¡i bá» cÃ¡c tháº» HTML vÃ  khoáº£ng tráº¯ng thá»«a
    """
    soup = BeautifulSoup(html, "html.parser")  # PhÃ¢n tÃ­ch cÃº phÃ¡p HTML
    return re.sub(r"\n\n+", "\n\n", soup.text).strip()  # XÃ³a khoáº£ng tráº¯ng vÃ  dÃ²ng trá»‘ng thá»«a

def crawl_web(url_data):
    """
    HÃ m crawl dá»¯ liá»‡u tá»« URL vá»›i cháº¿ Ä‘á»™ Ä‘á»‡ quy
    Args:
        url_data (str): URL gá»‘c Ä‘á»ƒ báº¯t Ä‘áº§u crawl
    Returns:
        list: Danh sÃ¡ch cÃ¡c Document object, má»—i object chá»©a ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c chia nhá»
              vÃ  metadata tÆ°Æ¡ng á»©ng
    """
    # Táº¡o loader vá»›i Ä‘á»™ sÃ¢u tá»‘i Ä‘a lÃ  4 cáº¥p
    loader = RecursiveUrlLoader(url=url_data, extractor=bs4_extractor, max_depth=4)
    docs = loader.load()  # Táº£i ná»™i dung
    print('length: ', len(docs))  # In sá»‘ lÆ°á»£ng tÃ i liá»‡u Ä‘Ã£ táº£i
    
    return docs