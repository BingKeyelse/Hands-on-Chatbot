import os
import json
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain.schema import Document
from dotenv import load_dotenv
from uuid import uuid4
# from crawl import crawl_web
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader
)
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import RecursiveUrlLoader, WebBaseLoader
from bs4 import BeautifulSoup

import nltk
nltk.download('averaged_perceptron_tagger_eng')

def load_data_from_local(filename: str, directory: str):
    """
    Tá»± Ä‘á»™ng Ä‘á»c file vÄƒn báº£n theo Ä‘á»‹nh dáº¡ng báº±ng LangChain loader.
    Há»— trá»£: PDF, TXT, DOCX, CSV, MD
    """
    file_path = os.path.join(directory, filename)
    ext = filename.lower().split('.')[-1]

    # Chá»n loader phÃ¹ há»£p
    if ext == 'pdf':
        # loader = PyPDFLoader(file_path)
        # docs = loader.load()

        # # ğŸ§  Gá»™p toÃ n bá»™ ná»™i dung cÃ¡c trang láº¡i thÃ nh 1 document duy nháº¥t
        # merged_text = "\n".join([d.page_content for d in docs])
        # documents = [
        #     Document(
        #         page_content=merged_text,
        #         metadata={"source": file_path}
        #     )
        # ]
        loader = PyPDFLoader(file_path)
        # KhÃ´ng gá»™p ná»¯a, Ä‘á»ƒ loader tráº£ vá» cÃ¡c trang riÃªng biá»‡t
        documents = loader.load()
    elif ext == 'txt':
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
    elif ext == 'docx':
        loader = UnstructuredWordDocumentLoader(file_path)
        documents = loader.load()
    elif ext == 'csv':
        loader = CSVLoader(file_path)
        documents = loader.load()
    elif ext == 'md':
        loader = UnstructuredMarkdownLoader(file_path)
        documents = loader.load()
    else:
        raise ValueError(f"âŒ Äá»‹nh dáº¡ng file '{ext}' chÆ°a Ä‘Æ°á»£c há»— trá»£.")

    # Load tÃ i liá»‡u (tráº£ vá» list Document)
    # documents = loader.load()
    return documents, filename, ext

def bs4_extractor(html: str) -> str:

    soup = BeautifulSoup(html, "html.parser")  # PhÃ¢n tÃ­ch cÃº phÃ¡p HTML
    return re.sub(r"\n\n+", "\n\n", soup.text).strip()  # XÃ³a khoáº£ng tráº¯ng vÃ  dÃ²ng trá»‘ng thá»«a

def crawl_web(url_data):

    # Táº¡o loader vá»›i Ä‘á»™ sÃ¢u tá»‘i Ä‘a lÃ  4 cáº¥p
    loader = RecursiveUrlLoader(url=url_data, extractor=bs4_extractor, max_depth=4)
    docs = loader.load()  # Táº£i ná»™i dung
    print('length: ', len(docs))  # In sá»‘ lÆ°á»£ng tÃ i liá»‡u Ä‘Ã£ táº£i
    
    return docs