# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from langchain.tools.retriever import create_retriever_tool  # T·∫°o c√¥ng c·ª• t√¨m ki·∫øm
from langchain_openai import ChatOpenAI  # Model ng√¥n ng·ªØ OpenAI
from langchain.agents import AgentExecutor, create_react_agent  # T·∫°o v√† th·ª±c thi agent
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # X·ª≠ l√Ω prompt
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate # X·ª≠ l√Ω prompt
from seed_data import connect_to_milvus  # K·∫øt n·ªëi v·ªõi Milvus
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler  # X·ª≠ l√Ω callback cho Streamlit
from langchain_community.chat_message_histories import StreamlitChatMessageHistory  # L∆∞u tr·ªØ l·ªãch s·ª≠ chat
from langchain.retrievers import EnsembleRetriever  # K·∫øt h·ª£p nhi·ªÅu retriever
from langchain_community.retrievers import BM25Retriever  # Retriever d·ª±a tr√™n BM25
from langchain_core.documents import Document  # L·ªõp Document
from pymilvus import Collection
from langchain_community.chat_models import ChatLlamaCpp
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory 
from langchain.agents import initialize_agent, AgentType

from langchain.tools import Tool # <--- Th√™m Tool th·ªß c√¥ng
from langchain.agents import initialize_agent, AgentType 
from langchain.memory import ConversationBufferMemory 
import streamlit as st # B·∫Øt bu·ªôc ph·∫£i import Streamlit

# from dotenv import load_dotenv

# load_dotenv()

def get_retriever() -> EnsembleRetriever:
    """
    T·∫°o m·ªôt ensemble retriever k·∫øt h·ª£p vector search (Milvus) v√† BM25
    Returns:
        EnsembleRetriever: Retriever k·∫øt h·ª£p v·ªõi t·ª∑ tr·ªçng:
            - 70% Milvus vector search (k=4 k·∫øt qu·∫£)
            - 30% BM25 text search (k=4 k·∫øt qu·∫£)
    Ch√∫ √Ω:
        - Y√™u c·∫ßu Milvus server ƒëang ch·∫°y t·∫°i localhost:19530
        - Collection 'database' ph·∫£i t·ªìn t·∫°i trong Milvus
        - BM25 ƒë∆∞·ª£c kh·ªüi t·∫°o t·ª´ 100 document ƒë·∫ßu ti√™n trong Milvus
    """

    # K·∫øt n·ªëi v·ªõi Milvus v√† t·∫°o vector retriever
    vectorstore = connect_to_milvus('http://localhost:19530', 'database')

    # Retriever similarity
    milvus_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 4})

    # Lay lai data documents de lam input cho BM25
    documents = [Document(page_content=doc.page_content, metadata=doc.metadata)
                 for doc in vectorstore.similarity_search("", k=1000)]

    # T·∫°o BM25 retriever t·ª´ to√†n b·ªô documents
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = 4

    # K·∫øt h·ª£p hai retriever v·ªõi t·ª∑ tr·ªçng
    ensemble_retriever = EnsembleRetriever(
        retrievers=[milvus_retriever, bm25_retriever],
        weights=[0.7, 0.3]
    )
    return bm25_retriever

# --- KH·ªûI T·∫†O LLM (B·∫ÆT BU·ªòC PH·∫¢I CACHE ƒê·ªÇ TR√ÅNH LOAD VRAM) ---
# T√™n h√†m ƒë√£ ƒë∆∞·ª£c thay ƒë·ªïi ƒë·ªÉ kh·ªõp v·ªõi LLM
# @st.cache_resource(show_spinner="ƒêang t·∫£i Model LlamaCpp v√†o GPU P100 (Ch·ªâ l·∫ßn ƒë·∫ßu)... üöÄ")
# def get_local_llm() -> ChatLlamaCpp:
#     """
#     T·∫£i m√¥ h√¨nh ChatLlamaCpp v√†o VRAM GPU m·ªôt l·∫ßn duy nh·∫•t.
#     """
#     # LLM s·∫Ω ƒë∆∞·ª£c gi·ªØ trong VRAM sau l·∫ßn load ƒë·∫ßu ti√™n
#     llm = ChatLlamaCpp(
#         model_path='vietnamese-llama2-7b-40gb.Q8_0.gguf',
#         temperature=0,
#         streaming=True,
#         n_ctx=4096,
#         # THAM S·ªê C·ª∞C K·ª≤ QUAN TR·ªåNG CHO GPU P100
#         n_gpu_layers=33, # ƒê·∫£m b·∫£o to√†n b·ªô model ƒë∆∞·ª£c offload l√™n P100 (cho Mistral 7B)
#         verbose=True
#     )
#     return llm

def get_local_llm() -> ChatOpenAI:
    """
    K·∫øt n·ªëi t·ªõi m√¥ h√¨nh LLM ƒë√£ ƒë∆∞·ª£c expose qua API Server.
    S·ª≠ d·ª•ng ChatOpenAI ƒë·ªÉ g·ªçi API.
    """
    
    # 1. Khai b√°o URL c·ªßa server API ƒë√£ ch·∫°y:
    # Do b·∫°n ch·∫°y tr√™n 0.0.0.0:8001, b·∫°n c√≥ th·ªÉ d√πng ƒë·ªãa ch·ªâ IP c·ª• th·ªÉ c·ªßa m√°y ch·ªß
    # (v√≠ d·ª•: 192.168.1.60) ho·∫∑c 0.0.0.0/localhost n·∫øu g·ªçi t·ª´ c√πng m√°y.
    # S·ª≠ d·ª•ng IP m·∫°ng n·ªôi b·ªô c·ªßa b·∫°n (v√≠ d·ª•: 192.168.1.60) ƒë·ªÉ c√°c m√°y kh√°c c√≥ th·ªÉ g·ªçi.
    
    BASE_URL = "http://127.0.0.1:8001/v1"  # Ho·∫∑c "http://192.168.1.60:8001/v1"

    # 2. Kh·ªüi t·∫°o ChatOpenAI client
    # llama_cpp.server m√¥ ph·ªèng API c·ªßa OpenAI.
    llm = ChatOpenAI(
        model="vietnamese-llama2-7b-40gb.Q8_0.gguf", # T√™n model t√πy √Ω, mi·ªÖn l√† server ƒëang ch·∫°y
        openai_api_base=BASE_URL,
        openai_api_key="sk-not-required",  # Kh√≥a API kh√¥ng c·∫ßn thi·∫øt cho server n·ªôi b·ªô
        temperature=0,
        streaming=True,
        # C√°c tham s·ªë kh√°c nh∆∞ n_ctx, n_gpu_layers KH√îNG ƒë∆∞·ª£c truy·ªÅn ·ªü ƒë√¢y, 
        # v√¨ ch√∫ng ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh c·∫•u h√¨nh tr√™n server (b·∫±ng l·ªánh --n_gpu_layers 33)
    )
    
    return llm


# T·∫°o c√¥ng c·ª• t√¨m ki·∫øm cho agent
def get_tools(retriever):
    """T·∫°o Tool th·ªß c√¥ng t·ª´ Retriever ƒë·ªÉ tr√°nh l·ªói tham s·ªë"""
    
    # H√†m search_function ch·ªâ nh·∫≠n m·ªôt bi·∫øn ƒë·∫ßu v√†o duy nh·∫•t (query)
    def search_function(query: str):
        # Tr·∫£ v·ªÅ k·∫øt qu·∫£ search d∆∞·ªõi d·∫°ng chu·ªói
        docs = retriever.invoke(query)
        return "\n\n".join([doc.page_content for doc in docs])
    
    # T·∫°o Tool
    return [
        Tool(
            name="find",
            func=search_function,
            # TH√äM H∆Ø·ªöNG D·∫™N B·∫∞NG TI·∫æNG VI·ªÜT
            description="C√¥ng c·ª• n√†y d√πng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu Milvus. H√£y s·ª≠ d·ª•ng n√≥ tr∆∞·ªõc khi tr·∫£ l·ªùi b·∫•t k·ª≥ c√¢u h·ªèi n√†o. D√πng ng√¥n ng·ªØ Ti·∫øng Vi·ªát khi ƒë·∫∑t c√¢u h·ªèi t√¨m ki·∫øm (Action Input)."
        )
    ]

# @st.cache_resource(show_spinner="Khoi tao get_llm_and_agent lan dau üöÄ")
# def get_llm_and_agent(_retriever, llm) -> AgentExecutor:
#     """
#     Kh·ªüi t·∫°o Language Model v√† Agent v·ªõi c·∫•u h√¨nh c·ª• th·ªÉ
#     Args:
#         _retriever: Retriever ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin
#     Returns:
#         AgentExecutor: Agent ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh v·ªõi:
#             - Model: GPT-4
#             - Temperature: 0
#             - Streaming: Enabled
#             - Custom system prompt
#     Ch√∫ √Ω:
#         - Y√™u c·∫ßu OPENAI_API_KEY ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh
#         - Agent ƒë∆∞·ª£c thi·∫øt l·∫≠p v·ªõi t√™n "ChatchatAI"
#         - S·ª≠ d·ª•ng chat history ƒë·ªÉ duy tr√¨ ng·ªØ c·∫£nh h·ªôi tho·∫°i
#     """
    
    
#     # Thi·∫øt l·∫≠p prompt template cho agent
#     system = """You are an expert at AI. Your name is ChatchatAI.
    
#         You MUST always first use the provided 'find' tool to search for context 
#         and detailed information before attempting to answer ANY user question.
#         Only answer the question based on the facts you retrieve.
#         If the tool returns no relevant information, state that you cannot find the answer.
#         """
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", system),
#         MessagesPlaceholder(variable_name="chat_history"),
#         ("human", "{input}"),
#         MessagesPlaceholder(variable_name="agent_scratchpad"),
#     ])

#     # T·∫°o v√† tr·∫£ v·ªÅ agent
#     agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
#     return AgentExecutor(agent=agent, tools=tools, verbose=False)
def get_llm_and_agent(_retriever, llm) -> AgentExecutor:
    
    # 1. KH·ªûI T·∫†O TOOLS
    tools = get_tools(_retriever) # <--- G·ªåI H√ÄM N√ÄY
    
    # 2. KH·ªûI T·∫†O MEMORY
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    # 3. KH·ªûI T·∫†O AGENT
    return initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
        verbose=False, # ƒê√£ t·∫Øt verbose
        memory=memory, 
        handle_parsing_errors=True
    )

# # Kh·ªüi t·∫°o retriever v√† agent
# retriever = get_retriever()
# llm= get_local_llm()
# agent_executor = get_llm_and_agent(retriever, llm)