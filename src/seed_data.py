import os
import json
from langchain_openai import OpenAIEmbeddings
from langchain_milvus import Milvus
from langchain.schema import Document
from dotenv import load_dotenv
from uuid import uuid4
# from crawl import crawl_web
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import time
from datetime import datetime
from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility


def load_data_from_local(filename: str, directory: str):
    """
    Tá»± Ä‘á»™ng Ä‘á»c file vÄƒn báº£n theo Ä‘á»‹nh dáº¡ng báº±ng LangChain loader.
    Há»— trá»£: PDF, TXT, DOCX, CSV, MD
    """
    file_path = os.path.join(directory, filename)
    ext = filename.lower().split('.')[-1]

    # Chá»n loader phÃ¹ há»£p
    if ext == 'pdf':
        loader = PyPDFLoader(file_path)
    elif ext == 'txt':
        loader = TextLoader(file_path, encoding='utf-8')
    elif ext == 'docx':
        loader = UnstructuredWordDocumentLoader(file_path)
    elif ext == 'csv':
        loader = CSVLoader(file_path)
    elif ext == 'md':
        loader = UnstructuredMarkdownLoader(file_path)
    else:
        raise ValueError(f"âŒ Äá»‹nh dáº¡ng file '{ext}' chÆ°a Ä‘Æ°á»£c há»— trá»£.")

    # Load tÃ i liá»‡u (tráº£ vá» list Document)
    documents = loader.load()
    return documents, filename, ext

def seed_milvus_local(URI_link: str, collection_name: str, filenames: list, directory: str) -> Milvus:
    # Khá»Ÿi táº¡o model embeddings
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    # # Káº¿t ná»‘i Milvus
    # connections.connect("default", uri=URI_link)

    # # Táº¡o schema náº¿u chÆ°a cÃ³ collection
    # if not utility.has_collection(collection_name):
    #     print(f"ğŸ“¦ Collection '{collection_name}' chÆ°a tá»“n táº¡i â€” táº¡o má»›i...")

    #     fields = [
    #         FieldSchema(name="pk", dtype=DataType.VARCHAR, is_primary=True, max_length=64),  # Ä‘á»•i tá»« id â†’ pk
    #         FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=384),
    #         FieldSchema(name="doc_name", dtype=DataType.VARCHAR, max_length=255),
    #         FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),
    #         FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=255),
    #         FieldSchema(name="date_update", dtype=DataType.VARCHAR, max_length=20)
    #     ]

    #     schema = CollectionSchema(fields=fields, description="Local text embedding storage")
    #     collection = Collection(name=collection_name, schema=schema)
    #     print(f"âœ… ÄÃ£ táº¡o collection '{collection_name}' thÃ nh cÃ´ng.")
    # else:
    #     collection = Collection(collection_name)
    #     print(f"âœ… Collection '{collection_name}' Ä‘Ã£ tá»“n táº¡i.")

    # Khá»Ÿi táº¡o Milvus vectorstore (khÃ´ng Ã©p consistency_level)
    vectorstore = Milvus(
        embedding_function=embeddings,
        connection_args={"uri": URI_link},
        collection_name=collection_name,
        drop_old=False
    )

    # Láº¥y thá»i gian hiá»‡n táº¡i
    current_time = datetime.now().strftime("%Y-%m-%d")

    # Xá»­ lÃ½ tá»«ng file trong danh sÃ¡ch
    for filename in filenames:
        local_data, doc_name, type_doc = load_data_from_local(filename, directory)

        # âœ‚ï¸ Chia nhá» vÄƒn báº£n
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=500)
        all_splits = text_splitter.split_documents(local_data)

        # Táº¡o danh sÃ¡ch Document
        documents = [
            Document(
                page_content=doc.page_content if hasattr(doc, "page_content") else doc.get("page_content", ""),
                metadata={
                    'doc_name': doc_name or '',
                    'content_type': type_doc or '',
                    'source': directory or '',
                    'date_update': current_time
                }
            )
            for doc in all_splits
        ]

        uuids = [str(uuid4()) for _ in range(len(documents))]

        # # âš™ï¸ Kiá»ƒm tra xem doc_name + content_type Ä‘Ã£ tá»“n táº¡i chÆ°a báº±ng query
        # try:
        #     collection.load()
        #     expr = f'doc_name == "{doc_name}" and content_type == "{type_doc}"'
        #     result = collection.query(expr=expr, output_fields=["id"])
        # except Exception as e:
        #     print(f"âš ï¸ KhÃ´ng thá»ƒ query collection: {e}")
        #     result = []

        # if len(result) > 0:
        #     print(f"ğŸ” PhÃ¡t hiá»‡n tÃ i liá»‡u trÃ¹ng '{doc_name}' ({type_doc}) â€” xÃ³a vÃ  cáº­p nháº­t láº¡i...")
        #     collection.delete(expr)
        #     collection.flush()

        # ğŸ§© ThÃªm má»›i hoáº·c cáº­p nháº­t document
        vectorstore.add_documents(documents=documents, ids=uuids)
        print(f"âœ… ÄÃ£ lÆ°u '{doc_name}' ({type_doc}) vÃ o collection '{collection_name}'")

    return vectorstore